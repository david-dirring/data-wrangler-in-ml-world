{
  
    
        "post0": {
            "title": "Title",
            "content": "This notebook is a script to run related to this blog post . To get started in Colab with BigQuery, you can start here . from google.colab import auth auth.authenticate_user() print(&#39;Authenticated&#39;) . Authenticated . from google.colab import syntax . The code below runs well in the https://console.cloud.google.com/bigquery UI. I usually don&#39;t use Jupyter notebooks to run BigQuery, but the script below should work for you if you change the parameters at the top. It takes less than 2 minutes to run. You&#39;ll see a failure because it detected a data issue. If you change &quot;mess_up_data&quot; from 1 to 0, then it won&#39;t fail. . %%bigquery --project blog-examples declare auc_min_to_fail float64 ; declare sql string; declare auc float64; declare mess_up_data int64; declare project_name string; declare dataset_name string; declare table_name string; declare main_table_name string; declare model_name string; declare train_table_name string; declare test_inference_table_name string; set project_name = &#39;blog-examples&#39;; #put your project name here set dataset_name = &#39;data_drift&#39;; #put your dataset name here, you&#39;ll likely need to create it if you don&#39;t have it already set table_name = &#39;data_drift_illustrated&#39;; set auc_min_to_fail = .65; set mess_up_data = 1; # 1 means run update statement to mess up data; 0 means dont mess up data #### inputs are above, should not need to modify anything below #### set main_table_name = &quot;`&quot; || project_name || &quot;.&quot; || dataset_name || &quot;.&quot; || table_name || &quot;`&quot;; set train_table_name = &quot;`&quot; || project_name || &quot;.&quot; || dataset_name || &quot;.&quot; || table_name || &quot;_train`&quot;; set test_inference_table_name = &quot;`&quot; || project_name || &quot;.&quot; || dataset_name || &quot;.&quot; || table_name || &quot;_test_inference`&quot; ; set model_name = &quot;`&quot; || project_name || &quot;.&quot; || dataset_name || &quot;.&quot; || table_name || &quot;_model`&quot;; set sql = FORMAT(&quot;&quot;&quot; #### I am grabbing a dataset from public bigquery. Historical liquor sales in Iowa. Let&#39;s pretend we work in Iowa at govt agency where we need to forecast liquor sales for whatever reason #### create or replace table %s OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours as with liquor_sales_wk_zip_cat_vendor as ( SELECT DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) as sale_wk_bgn_dt , DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) + 6 as sale_wk_end_dt , cast(zip_code as string) as store_zip_code # making sure this is a string, as I want BQML to see it as a categorical , category_name , vendor_name , item_description , sum(volume_sold_gallons) as volume_sold_gallons FROM `bigquery-public-data.iowa_liquor_sales.sales` where date between (select max(date) - ((7*104)+28) from `bigquery-public-data.iowa_liquor_sales.sales`) --getting 104 weeks, starting from 4 weeks ago and (select max(date) - 28 from `bigquery-public-data.iowa_liquor_sales.sales`) --max date will be 4ish weeks ago group by 1,2,3,4,5,6 # order by 1 desc # don&#39;t put order by in a CTE, not a good practice, doing here for illustrative purposes ) #### I actually recommend to throw the data into a tool like Tableau at this point so you can get to know the data, but I will spare you of a David Going On About Tableau rant for the time being. , check_dates as ( select distinct case when row_number() over(order by sale_wk_end_dt desc) &lt;= 3 then &#39;test-inference&#39; # this is the most recent 3 week period when row_number() over(order by sale_wk_end_dt desc) &lt;= 6 then &#39;validation&#39; # this is the holdout set, I use this as unseen data, test my model against this dataset when row_number() over(order by sale_wk_end_dt desc) &lt;= 9 then &#39;test&#39; # as I train a model, I&#39;ll test against this dataset during the epochs else &#39;train&#39; --this is the training data end as train_or_test , sale_wk_bgn_dt, sale_wk_end_dt from ( select distinct sale_wk_bgn_dt, sale_wk_end_dt # rolling up weeks so I can easily get row_number from liquor_sales_wk_zip_cat_vendor ) order by sale_wk_end_dt desc ) select s.* , d.train_or_test from liquor_sales_wk_zip_cat_vendor s inner join check_dates d on s.sale_wk_end_dt = d.sale_wk_end_dt ; &quot;&quot;&quot; , (select main_table_name) ); select sql; execute immediate(sql); set sql = FORMAT(&quot;&quot;&quot; #### this is the table we would normally use in training #### create or replace table %s OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours as select * from %s where train_or_test in (&#39;train&#39;,&#39;test&#39;,&#39;validation&#39;) ; &quot;&quot;&quot; , (select train_table_name) , (select main_table_name) ); select sql; execute immediate(sql); set sql = FORMAT(&quot;&quot;&quot; #### this is the table we would use in our real &quot;test&quot;, where we have to load in the data live from the source, usually this is a complicated process involving tons of scripts to pull the features. #### create or replace table %s OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours as select * from %s where train_or_test in (&#39;test-inference&#39;) ; &quot;&quot;&quot; , (select test_inference_table_name) , (select main_table_name) ); select sql; execute immediate(sql); set sql = FORMAT(&quot;&quot;&quot; #### we update the category_name to null... simulating an issue with an upstream data source #### update %s set category_name = null where 1=%s #if user puts 1, then we update because 1 always equals 1 ; &quot;&quot;&quot; , (select test_inference_table_name) , (select cast(mess_up_data as string)) #I guess we have to convert to string to put inside a string ); select sql; execute immediate(sql); set sql = FORMAT(&quot;&quot;&quot; CREATE OR REPLACE MODEL %s OPTIONS( MODEL_TYPE=&#39;LOGISTIC_REG&#39; , input_label_cols=[&#39;label&#39;] ) AS with get_train as ( select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model. In prod I actually grab these columns using dynamic sql , cast(0 as int64) as label --mark the train records as 0 from %s order by rand() limit 5000 ) , get_test_inference as ( select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model. In prod I actually grab these columns using dynamic sql , cast(1 as int64) as label --mark the test-inference records as 1 from %s order by rand() limit 5000 ) select * from get_train union all select * from get_test_inference ; &quot;&quot;&quot; , (select model_name) , (select train_table_name) , (select test_inference_table_name) ); select sql; execute immediate(sql); set sql = FORMAT(&quot;&quot;&quot; create or replace temp table auc_and_top_contributors as SELECT case when e.roc_auc &lt;= %s then &#39;no issues&#39; else &#39;yikes, dig in!&#39; end as is_there_an_issue , round(e.roc_auc,4) as auc , w.processed_input as feature_name , round(w.weight,4) as weight , cw.category , round(cw.weight,4) as categorical_weight FROM ML.WEIGHTS (MODEL %s ,STRUCT(true AS standardize) ) w , unnest(category_weights) as cw , ML.EVALUATE(MODEL %s ) e order by coalesce(abs(w.weight),cw.weight) desc limit 8 ; &quot;&quot;&quot; , (select cast(auc_min_to_fail as string)) , (select model_name) , (select model_name) ); select sql; execute immediate(sql); select * from auc_and_top_contributors; if (SELECT distinct auc from auc_and_top_contributors) &gt; auc_min_to_fail then --auc_min_to_fail then --if auc is above x, then fail the job select ERROR(&quot;YIKES! you have a data issue!&quot;); end if; . Executing query with job ID: cf47eecb-42e5-416e-a978-71ce45d59f2e Query executing: 100.73s . ERROR: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/blog-examples/queries/cf47eecb-42e5-416e-a978-71ce45d59f2e?maxResults=0&amp;timeoutMs=400&amp;location=US: Query error: YIKES! you have a data issue! at [198:5] (job ID: cf47eecb-42e5-416e-a978-71ce45d59f2e) --Query Job SQL Follows-- | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | 1:declare auc_min_to_fail float64 ; 2:declare sql string; 3:declare auc float64; 4:declare mess_up_data int64; 5:declare project_name string; declare dataset_name string; declare table_name string; declare main_table_name string; declare model_name string; declare train_table_name string; declare test_inference_table_name string; 6: 7:set project_name = &#39;blog-examples&#39;; #put your project name here 8:set dataset_name = &#39;data_drift&#39;; #put your dataset name here, you&#39;ll likely need to create it if you don&#39;t have it already 9:set table_name = &#39;data_drift_illustrated&#39;; 10:set auc_min_to_fail = .65; 11:set mess_up_data = 1; # 1 means run update statement to mess up data; 0 means dont mess up data 12: 13: 14:#### inputs are above, should not need to modify anything below #### 15: 16:set main_table_name = &#34;`&#34; || project_name || &#34;.&#34; || dataset_name || &#34;.&#34; || table_name || &#34;`&#34;; 17:set train_table_name = &#34;`&#34; || project_name || &#34;.&#34; || dataset_name || &#34;.&#34; || table_name || &#34;_train`&#34;; 18:set test_inference_table_name = &#34;`&#34; || project_name || &#34;.&#34; || dataset_name || &#34;.&#34; || table_name || &#34;_test_inference`&#34; ; 19:set model_name = &#34;`&#34; || project_name || &#34;.&#34; || dataset_name || &#34;.&#34; || table_name || &#34;_model`&#34;; 20: 21: 22:set sql = FORMAT(&#34;&#34;&#34; 23: #### I am grabbing a dataset from public bigquery. Historical liquor sales in Iowa. Let&#39;s pretend we work in Iowa at govt agency where we need to forecast liquor sales for whatever reason #### 24: create or replace table %s 25: OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours 26: as 27: with liquor_sales_wk_zip_cat_vendor as ( 28: SELECT 29: DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) as sale_wk_bgn_dt 30: , DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) + 6 as sale_wk_end_dt 31: , cast(zip_code as string) as store_zip_code # making sure this is a string, as I want BQML to see it as a categorical 32: , category_name 33: , vendor_name 34: , item_description 35: , sum(volume_sold_gallons) as volume_sold_gallons 36: FROM `bigquery-public-data.iowa_liquor_sales.sales` 37: where date between 38: (select max(date) - ((7*104)+28) from `bigquery-public-data.iowa_liquor_sales.sales`) --getting 104 weeks, starting from 4 weeks ago 39: and (select max(date) - 28 from `bigquery-public-data.iowa_liquor_sales.sales`) --max date will be 4ish weeks ago 40: group by 1,2,3,4,5,6 41: # order by 1 desc # don&#39;t put order by in a CTE, not a good practice, doing here for illustrative purposes 42: ) 43: #### I actually recommend to throw the data into a tool like Tableau at this point so you can get to know the data, but I will spare you of a David Going On About Tableau rant for the time being. 44: 45: , check_dates as ( 46: select distinct 47: case when row_number() over(order by sale_wk_end_dt desc) &lt;= 3 then &#39;test-inference&#39; # this is the most recent 3 week period 48: when row_number() over(order by sale_wk_end_dt desc) &lt;= 6 then &#39;validation&#39; # this is the holdout set, I use this as unseen data, test my model against this dataset 49: when row_number() over(order by sale_wk_end_dt desc) &lt;= 9 then &#39;test&#39; # as I train a model, I&#39;ll test against this dataset during the epochs 50: else &#39;train&#39; --this is the training data 51: end as train_or_test 52: , sale_wk_bgn_dt, sale_wk_end_dt 53: from ( 54: select distinct sale_wk_bgn_dt, sale_wk_end_dt # rolling up weeks so I can easily get row_number 55: from liquor_sales_wk_zip_cat_vendor 56: ) 57: order by sale_wk_end_dt desc 58: 59: ) 60: 61: select s.* , d.train_or_test 62: from liquor_sales_wk_zip_cat_vendor s 63: inner join check_dates d on s.sale_wk_end_dt = d.sale_wk_end_dt 64: 65: ; 66:&#34;&#34;&#34; 67:, (select main_table_name) 68:); 69: 70:select sql; 71:execute immediate(sql); 72: 73: 74:set sql = FORMAT(&#34;&#34;&#34; 75: #### this is the table we would normally use in training #### 76: create or replace table %s 77: OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours 78: as 79: select * from %s 80: where train_or_test in (&#39;train&#39;,&#39;test&#39;,&#39;validation&#39;) 81: ; 82:&#34;&#34;&#34; 83:, (select train_table_name) 84:, (select main_table_name) 85: 86:); 87: 88:select sql; 89:execute immediate(sql); 90: 91: 92:set sql = FORMAT(&#34;&#34;&#34; 93: #### this is the table we would use in our real &#34;test&#34;, where we have to load in the data live from the source, usually this is a complicated process involving tons of scripts to pull the features. #### 94: create or replace table %s 95: OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours 96: as 97: select * from %s 98: where train_or_test in (&#39;test-inference&#39;) 99: ; 100:&#34;&#34;&#34; 101:, (select test_inference_table_name) 102:, (select main_table_name) 103: 104:); 105: 106:select sql; 107:execute immediate(sql); 108: 109: 110:set sql = FORMAT(&#34;&#34;&#34; 111: #### we update the category_name to null... simulating an issue with an upstream data source #### 112: update %s 113: set category_name = null 114: where 1=%s #if user puts 1, then we update because 1 always equals 1 115: ; 116:&#34;&#34;&#34; 117:, (select test_inference_table_name) 118:, (select cast(mess_up_data as string)) #I guess we have to convert to string to put inside a string 119:); 120: 121:select sql; 122:execute immediate(sql); 123: 124: 125: 126: 127:set sql = FORMAT(&#34;&#34;&#34; 128: 129: CREATE OR REPLACE MODEL %s 130: OPTIONS( 131: MODEL_TYPE=&#39;LOGISTIC_REG&#39; 132: , input_label_cols=[&#39;label&#39;] 133: ) AS 134: with get_train as ( 135: select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model. In prod I actually grab these columns using dynamic sql 136: , cast(0 as int64) as label --mark the train records as 0 137: from %s 138: order by rand() 139: limit 5000 140: ) 141: 142: , get_test_inference as ( 143: select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model. In prod I actually grab these columns using dynamic sql 144: , cast(1 as int64) as label --mark the test-inference records as 1 145: from %s 146: order by rand() 147: limit 5000 148: ) 149: 150: select * from get_train 151: union all 152: select * from get_test_inference 153: ; 154: 155:&#34;&#34;&#34; 156:, (select model_name) 157:, (select train_table_name) 158:, (select test_inference_table_name) 159: 160:); 161: 162:select sql; 163:execute immediate(sql); 164: 165: 166: 167: 168:set sql = FORMAT(&#34;&#34;&#34; 169: 170: create or replace temp table auc_and_top_contributors as 171: SELECT 172: case when e.roc_auc &lt;= %s then &#39;no issues&#39; else &#39;yikes, dig in!&#39; end as is_there_an_issue 173: , round(e.roc_auc,4) as auc 174: , w.processed_input as feature_name 175: , round(w.weight,4) as weight 176: , cw.category 177: , round(cw.weight,4) as categorical_weight 178: FROM 179: ML.WEIGHTS (MODEL %s 180: ,STRUCT(true AS standardize) 181: ) w 182: , unnest(category_weights) as cw 183: , ML.EVALUATE(MODEL %s ) e 184: order by coalesce(abs(w.weight),cw.weight) desc 185: limit 8 186: ; 187:&#34;&#34;&#34; 188: , (select cast(auc_min_to_fail as string)) 189: , (select model_name) 190: , (select model_name) 191:); 192: 193:select sql; execute immediate(sql); 194: 195:select * from auc_and_top_contributors; 196: 197:if (SELECT distinct auc from auc_and_top_contributors) &gt; auc_min_to_fail then --auc_min_to_fail then --if auc is above x, then fail the job 198: select ERROR(&#34;YIKES! you have a data issue!&#34;); 199:end if; | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | . | .",
            "url": "https://david-dirring.github.io/data-wrangler-in-ml-world/2021/06/08/tackle_data_drift_with_bqml_bigquery_script.html",
            "relUrl": "/2021/06/08/tackle_data_drift_with_bqml_bigquery_script.html",
            "date": " • Jun 8, 2021"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://david-dirring.github.io/data-wrangler-in-ml-world/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is David Dirring. I am currently a Principal Data Scientist at The Home Depot in Atlanta, Georgia. . I have been wranging data, data storytelling, and helping internal customers understand their area for 13+ years. In the last 3 years, I began teaching myself data science and machine learning. I am very lucky that I get to work on fun ML projects at work. My tools of choice are – BigQuery (or any SQL), Tableau, Python, and Powerpoint. . I want to use this blog to help document my machine learning journey so that I can help others while also helping to solidify learnings and thoughts of my own. .",
          "url": "https://david-dirring.github.io/data-wrangler-in-ml-world/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://david-dirring.github.io/data-wrangler-in-ml-world/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}