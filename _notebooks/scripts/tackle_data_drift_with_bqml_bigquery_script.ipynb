{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tackle_data_drift_with_bqml_bigquery_script.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSZUc9oUUFNTI6C3UxXm6V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djgVmY-YtmFh"
      },
      "source": [
        "This notebook is a script to run related to this [blog post](https://david-dirring.github.io/data-wrangler-in-ml-world/fastpages/jupyter/bqml/gcp/data/machine%20learning/2021/06/08/_06_04_tackle_data_drift_with_bqml.html)\n",
        "\n",
        "To get started in Colab with BigQuery, you can start [here](https://colab.research.google.com/notebooks/bigquery.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLC4N8a4rmtb",
        "outputId": "5bd61147-48ce-4763-98e8-b07b60565612"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKng8Ti_ufaH"
      },
      "source": [
        "from google.colab import syntax"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlYNeSjLruVF"
      },
      "source": [
        "The code below runs well in the https://console.cloud.google.com/bigquery UI.  I usually don't use Jupyter notebooks to run BigQuery, but the script below should work for you if you change the parameters at the top. It takes less than 2 minutes to run. You'll see a failure because it detected a data issue. If you change \"mess_up_data\" from 1 to 0, then it won't fail.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvvf9X9ksBGe",
        "outputId": "d124102e-d6d1-4696-f48b-acac627143c8"
      },
      "source": [
        "%%bigquery --project blog-examples\n",
        "\n",
        "declare auc_min_to_fail float64 ;\n",
        "declare sql string;\n",
        "declare auc float64; \n",
        "declare mess_up_data int64; \n",
        "declare project_name string; declare dataset_name string; declare table_name string; declare main_table_name string; declare model_name string; declare train_table_name string; declare test_inference_table_name string;\n",
        "\n",
        "set project_name = 'blog-examples'; #put your project name here\n",
        "set dataset_name = 'data_drift';  #put your dataset name here, you'll likely need to create it if you don't have it already\n",
        "set table_name = 'data_drift_illustrated'; \n",
        "set auc_min_to_fail = .65;\n",
        "set mess_up_data = 1; # 1 means run update statement to mess up data; 0 means dont mess up data\n",
        "\n",
        "\n",
        "#### inputs are above, should not need to modify anything below ####\n",
        "\n",
        "set main_table_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"`\";\n",
        "set train_table_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"_train`\";\n",
        "set test_inference_table_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"_test_inference`\" ;\n",
        "set model_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"_model`\";\n",
        "\n",
        "\n",
        "set sql = FORMAT(\"\"\"\n",
        "    #### I am grabbing a dataset from public bigquery. Historical liquor sales in Iowa. Let's pretend we work in Iowa at govt agency where we need to forecast liquor sales for whatever reason  ####\n",
        "    create or replace table %s\n",
        "        OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours\n",
        "    as \n",
        "        with liquor_sales_wk_zip_cat_vendor as ( \n",
        "            SELECT \n",
        "                DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) as sale_wk_bgn_dt\n",
        "                , DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) + 6 as sale_wk_end_dt\n",
        "                , cast(zip_code as string) as store_zip_code # making sure this is a string, as I want BQML to see it as a categorical\n",
        "                , category_name\n",
        "                , vendor_name\n",
        "                , item_description\n",
        "                , sum(volume_sold_gallons) as volume_sold_gallons\n",
        "            FROM `bigquery-public-data.iowa_liquor_sales.sales`\n",
        "            where date between \n",
        "                (select max(date) - ((7*104)+28) from `bigquery-public-data.iowa_liquor_sales.sales`) --getting 104 weeks, starting from 4 weeks ago\n",
        "                and (select max(date) - 28  from `bigquery-public-data.iowa_liquor_sales.sales`) --max date will be 4ish weeks ago\n",
        "            group by 1,2,3,4,5,6\n",
        "            # order by 1 desc # don't put order by in a CTE, not a good practice, doing here for illustrative purposes\n",
        "        )\n",
        "        #### I actually recommend to throw the data into a tool like Tableau at this point so you can get to know the data, but I will spare you of a David Going On About Tableau rant for the time being.  \n",
        "\n",
        "        , check_dates as (\n",
        "            select distinct \n",
        "                case when row_number() over(order by sale_wk_end_dt desc) <= 3 then 'test-inference' # this is the most recent 3 week period\n",
        "                    when row_number() over(order by sale_wk_end_dt desc) <= 6 then 'validation' # this is the holdout set, I use this as unseen data, test my model against this dataset\n",
        "                    when row_number() over(order by sale_wk_end_dt desc) <= 9 then 'test' # as I train a model, I'll test against this dataset during the epochs\n",
        "                    else 'train' --this is the training data\n",
        "                end as train_or_test\n",
        "                , sale_wk_bgn_dt, sale_wk_end_dt\n",
        "            from (\n",
        "                select distinct sale_wk_bgn_dt, sale_wk_end_dt  # rolling up weeks so I can easily get row_number\n",
        "                from liquor_sales_wk_zip_cat_vendor\n",
        "            )\n",
        "            order by sale_wk_end_dt desc\n",
        "\n",
        "        )\n",
        "\n",
        "        select s.* , d.train_or_test \n",
        "        from liquor_sales_wk_zip_cat_vendor s\n",
        "            inner join check_dates d on s.sale_wk_end_dt = d.sale_wk_end_dt\n",
        "        \n",
        "        ; \n",
        "\"\"\"\n",
        ", (select main_table_name) \n",
        "); \n",
        "\n",
        "select sql;\n",
        "execute immediate(sql);\n",
        "\n",
        "\n",
        "set sql = FORMAT(\"\"\"\n",
        "    #### this is the table we would normally use in training ####\n",
        "    create or replace table %s\n",
        "        OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR))  #table will drop in 24 hours\n",
        "    as \n",
        "        select * from %s\n",
        "        where train_or_test in ('train','test','validation')\n",
        "    ;\n",
        "\"\"\"\n",
        ", (select train_table_name) \n",
        ", (select main_table_name) \n",
        "\n",
        "); \n",
        "\n",
        "select sql;\n",
        "execute immediate(sql);\n",
        "\n",
        "\n",
        "set sql = FORMAT(\"\"\"\n",
        "    #### this is the table we would use in our real \"test\", where we have to load in the data live from the source, usually this is a complicated process involving tons of scripts to pull the features. ####\n",
        "    create or replace table %s\n",
        "        OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR))  #table will drop in 24 hours\n",
        "    as \n",
        "        select * from %s\n",
        "        where train_or_test in ('test-inference')\n",
        "    ;\n",
        "\"\"\"\n",
        ", (select test_inference_table_name) \n",
        ", (select main_table_name) \n",
        "\n",
        "); \n",
        "\n",
        "select sql;\n",
        "execute immediate(sql);\n",
        "\n",
        "\n",
        "set sql = FORMAT(\"\"\"\n",
        "    #### we update the category_name to null... simulating an issue with an upstream data source ####\n",
        "    update %s\n",
        "    set category_name = null\n",
        "    where 1=%s #if user puts 1, then we update because 1 always equals 1\n",
        "    ;\n",
        "\"\"\"\n",
        ", (select test_inference_table_name) \n",
        ", (select cast(mess_up_data as string)) #I guess we have to convert to string to put inside a string\n",
        "); \n",
        "\n",
        "select sql;\n",
        "execute immediate(sql);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "set sql = FORMAT(\"\"\"\n",
        "\n",
        "    CREATE OR REPLACE MODEL %s\n",
        "    OPTIONS( \n",
        "        MODEL_TYPE='LOGISTIC_REG'\n",
        "        , input_label_cols=['label']\n",
        "    ) AS\n",
        "        with get_train as (\n",
        "            select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model.  In prod I actually grab these columns using dynamic sql\n",
        "                , cast(0 as int64) as label --mark the train records as 0  \n",
        "            from %s\n",
        "            order by rand() \n",
        "            limit 5000\n",
        "        )\n",
        "        \n",
        "        , get_test_inference as (\n",
        "            select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model.  In prod I actually grab these columns using dynamic sql\n",
        "                , cast(1 as int64) as label --mark the test-inference records as 1  \n",
        "            from %s\n",
        "            order by rand() \n",
        "            limit 5000\n",
        "        )\n",
        "\n",
        "        select * from get_train\n",
        "            union all \n",
        "        select * from get_test_inference\n",
        "    ; \n",
        "    \n",
        "\"\"\"\n",
        ", (select model_name) \n",
        ", (select train_table_name) \n",
        ", (select test_inference_table_name) \n",
        "\n",
        "); \n",
        "\n",
        "select sql;\n",
        "execute immediate(sql);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "set sql = FORMAT(\"\"\"\n",
        "\n",
        "    create or replace temp table auc_and_top_contributors as \n",
        "        SELECT\n",
        "            case when e.roc_auc <= %s then 'no issues' else 'yikes, dig in!' end as is_there_an_issue\n",
        "            , round(e.roc_auc,4) as auc      \n",
        "            , w.processed_input as feature_name\n",
        "            , round(w.weight,4) as weight\n",
        "            , cw.category\n",
        "            , round(cw.weight,4) as categorical_weight\n",
        "        FROM\n",
        "            ML.WEIGHTS (MODEL  %s \n",
        "                ,STRUCT(true AS standardize)\n",
        "            ) w\n",
        "            , unnest(category_weights) as cw\n",
        "            , ML.EVALUATE(MODEL  %s  ) e\n",
        "        order by coalesce(abs(w.weight),cw.weight) desc\n",
        "        limit 8\n",
        "    ; \n",
        "\"\"\"\n",
        "    , (select cast(auc_min_to_fail as string))\n",
        "    , (select model_name) \n",
        "    , (select model_name) \n",
        "); \n",
        "\n",
        "select sql; execute immediate(sql);\n",
        "\n",
        "select * from auc_and_top_contributors; \n",
        "\n",
        "if (SELECT distinct auc from auc_and_top_contributors) > auc_min_to_fail then --auc_min_to_fail then --if auc is above x, then fail the job\n",
        "    select ERROR(\"YIKES! you have a data issue!\");\n",
        "end if;\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executing query with job ID: cf47eecb-42e5-416e-a978-71ce45d59f2e\n",
            "Query executing: 100.73s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ERROR:\n",
            " 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/blog-examples/queries/cf47eecb-42e5-416e-a978-71ce45d59f2e?maxResults=0&timeoutMs=400&location=US: Query error: YIKES! you have a data issue! at [198:5]\n",
            "\n",
            "(job ID: cf47eecb-42e5-416e-a978-71ce45d59f2e)\n",
            "\n",
            "                                                                                                -----Query Job SQL Follows-----                                                                                                \n",
            "\n",
            "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n",
            "   1:declare auc_min_to_fail float64 ;\n",
            "   2:declare sql string;\n",
            "   3:declare auc float64; \n",
            "   4:declare mess_up_data int64; \n",
            "   5:declare project_name string; declare dataset_name string; declare table_name string; declare main_table_name string; declare model_name string; declare train_table_name string; declare test_inference_table_name string;\n",
            "   6:\n",
            "   7:set project_name = 'blog-examples'; #put your project name here\n",
            "   8:set dataset_name = 'data_drift';  #put your dataset name here, you'll likely need to create it if you don't have it already\n",
            "   9:set table_name = 'data_drift_illustrated'; \n",
            "  10:set auc_min_to_fail = .65;\n",
            "  11:set mess_up_data = 1; # 1 means run update statement to mess up data; 0 means dont mess up data\n",
            "  12:\n",
            "  13:\n",
            "  14:#### inputs are above, should not need to modify anything below ####\n",
            "  15:\n",
            "  16:set main_table_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"`\";\n",
            "  17:set train_table_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"_train`\";\n",
            "  18:set test_inference_table_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"_test_inference`\" ;\n",
            "  19:set model_name = \"`\" || project_name || \".\" || dataset_name || \".\" || table_name || \"_model`\";\n",
            "  20:\n",
            "  21:\n",
            "  22:set sql = FORMAT(\"\"\"\n",
            "  23:    #### I am grabbing a dataset from public bigquery. Historical liquor sales in Iowa. Let's pretend we work in Iowa at govt agency where we need to forecast liquor sales for whatever reason  ####\n",
            "  24:    create or replace table %s\n",
            "  25:        OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) #table will drop in 24 hours\n",
            "  26:    as \n",
            "  27:        with liquor_sales_wk_zip_cat_vendor as ( \n",
            "  28:            SELECT \n",
            "  29:                DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) as sale_wk_bgn_dt\n",
            "  30:                , DATE(TIMESTAMP_TRUNC(TIMESTAMP(DATE), WEEK)) + 6 as sale_wk_end_dt\n",
            "  31:                , cast(zip_code as string) as store_zip_code # making sure this is a string, as I want BQML to see it as a categorical\n",
            "  32:                , category_name\n",
            "  33:                , vendor_name\n",
            "  34:                , item_description\n",
            "  35:                , sum(volume_sold_gallons) as volume_sold_gallons\n",
            "  36:            FROM `bigquery-public-data.iowa_liquor_sales.sales`\n",
            "  37:            where date between \n",
            "  38:                (select max(date) - ((7*104)+28) from `bigquery-public-data.iowa_liquor_sales.sales`) --getting 104 weeks, starting from 4 weeks ago\n",
            "  39:                and (select max(date) - 28  from `bigquery-public-data.iowa_liquor_sales.sales`) --max date will be 4ish weeks ago\n",
            "  40:            group by 1,2,3,4,5,6\n",
            "  41:            # order by 1 desc # don't put order by in a CTE, not a good practice, doing here for illustrative purposes\n",
            "  42:        )\n",
            "  43:        #### I actually recommend to throw the data into a tool like Tableau at this point so you can get to know the data, but I will spare you of a David Going On About Tableau rant for the time being.  \n",
            "  44:\n",
            "  45:        , check_dates as (\n",
            "  46:            select distinct \n",
            "  47:                case when row_number() over(order by sale_wk_end_dt desc) <= 3 then 'test-inference' # this is the most recent 3 week period\n",
            "  48:                    when row_number() over(order by sale_wk_end_dt desc) <= 6 then 'validation' # this is the holdout set, I use this as unseen data, test my model against this dataset\n",
            "  49:                    when row_number() over(order by sale_wk_end_dt desc) <= 9 then 'test' # as I train a model, I'll test against this dataset during the epochs\n",
            "  50:                    else 'train' --this is the training data\n",
            "  51:                end as train_or_test\n",
            "  52:                , sale_wk_bgn_dt, sale_wk_end_dt\n",
            "  53:            from (\n",
            "  54:                select distinct sale_wk_bgn_dt, sale_wk_end_dt  # rolling up weeks so I can easily get row_number\n",
            "  55:                from liquor_sales_wk_zip_cat_vendor\n",
            "  56:            )\n",
            "  57:            order by sale_wk_end_dt desc\n",
            "  58:\n",
            "  59:        )\n",
            "  60:\n",
            "  61:        select s.* , d.train_or_test \n",
            "  62:        from liquor_sales_wk_zip_cat_vendor s\n",
            "  63:            inner join check_dates d on s.sale_wk_end_dt = d.sale_wk_end_dt\n",
            "  64:        \n",
            "  65:        ; \n",
            "  66:\"\"\"\n",
            "  67:, (select main_table_name) \n",
            "  68:); \n",
            "  69:\n",
            "  70:select sql;\n",
            "  71:execute immediate(sql);\n",
            "  72:\n",
            "  73:\n",
            "  74:set sql = FORMAT(\"\"\"\n",
            "  75:    #### this is the table we would normally use in training ####\n",
            "  76:    create or replace table %s\n",
            "  77:        OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR))  #table will drop in 24 hours\n",
            "  78:    as \n",
            "  79:        select * from %s\n",
            "  80:        where train_or_test in ('train','test','validation')\n",
            "  81:    ;\n",
            "  82:\"\"\"\n",
            "  83:, (select train_table_name) \n",
            "  84:, (select main_table_name) \n",
            "  85:\n",
            "  86:); \n",
            "  87:\n",
            "  88:select sql;\n",
            "  89:execute immediate(sql);\n",
            "  90:\n",
            "  91:\n",
            "  92:set sql = FORMAT(\"\"\"\n",
            "  93:    #### this is the table we would use in our real \"test\", where we have to load in the data live from the source, usually this is a complicated process involving tons of scripts to pull the features. ####\n",
            "  94:    create or replace table %s\n",
            "  95:        OPTIONS (expiration_timestamp=TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR))  #table will drop in 24 hours\n",
            "  96:    as \n",
            "  97:        select * from %s\n",
            "  98:        where train_or_test in ('test-inference')\n",
            "  99:    ;\n",
            " 100:\"\"\"\n",
            " 101:, (select test_inference_table_name) \n",
            " 102:, (select main_table_name) \n",
            " 103:\n",
            " 104:); \n",
            " 105:\n",
            " 106:select sql;\n",
            " 107:execute immediate(sql);\n",
            " 108:\n",
            " 109:\n",
            " 110:set sql = FORMAT(\"\"\"\n",
            " 111:    #### we update the category_name to null... simulating an issue with an upstream data source ####\n",
            " 112:    update %s\n",
            " 113:    set category_name = null\n",
            " 114:    where 1=%s #if user puts 1, then we update because 1 always equals 1\n",
            " 115:    ;\n",
            " 116:\"\"\"\n",
            " 117:, (select test_inference_table_name) \n",
            " 118:, (select cast(mess_up_data as string)) #I guess we have to convert to string to put inside a string\n",
            " 119:); \n",
            " 120:\n",
            " 121:select sql;\n",
            " 122:execute immediate(sql);\n",
            " 123:\n",
            " 124:\n",
            " 125:\n",
            " 126:\n",
            " 127:set sql = FORMAT(\"\"\"\n",
            " 128:\n",
            " 129:    CREATE OR REPLACE MODEL %s\n",
            " 130:    OPTIONS( \n",
            " 131:        MODEL_TYPE='LOGISTIC_REG'\n",
            " 132:        , input_label_cols=['label']\n",
            " 133:    ) AS\n",
            " 134:        with get_train as (\n",
            " 135:            select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model.  In prod I actually grab these columns using dynamic sql\n",
            " 136:                , cast(0 as int64) as label --mark the train records as 0  \n",
            " 137:            from %s\n",
            " 138:            order by rand() \n",
            " 139:            limit 5000\n",
            " 140:        )\n",
            " 141:        \n",
            " 142:        , get_test_inference as (\n",
            " 143:            select store_zip_code, category_name, vendor_name --grab the 3 fields that we use in the model.  In prod I actually grab these columns using dynamic sql\n",
            " 144:                , cast(1 as int64) as label --mark the test-inference records as 1  \n",
            " 145:            from %s\n",
            " 146:            order by rand() \n",
            " 147:            limit 5000\n",
            " 148:        )\n",
            " 149:\n",
            " 150:        select * from get_train\n",
            " 151:            union all \n",
            " 152:        select * from get_test_inference\n",
            " 153:    ; \n",
            " 154:    \n",
            " 155:\"\"\"\n",
            " 156:, (select model_name) \n",
            " 157:, (select train_table_name) \n",
            " 158:, (select test_inference_table_name) \n",
            " 159:\n",
            " 160:); \n",
            " 161:\n",
            " 162:select sql;\n",
            " 163:execute immediate(sql);\n",
            " 164:\n",
            " 165:\n",
            " 166:\n",
            " 167:\n",
            " 168:set sql = FORMAT(\"\"\"\n",
            " 169:\n",
            " 170:    create or replace temp table auc_and_top_contributors as \n",
            " 171:        SELECT\n",
            " 172:            case when e.roc_auc <= %s then 'no issues' else 'yikes, dig in!' end as is_there_an_issue\n",
            " 173:            , round(e.roc_auc,4) as auc      \n",
            " 174:            , w.processed_input as feature_name\n",
            " 175:            , round(w.weight,4) as weight\n",
            " 176:            , cw.category\n",
            " 177:            , round(cw.weight,4) as categorical_weight\n",
            " 178:        FROM\n",
            " 179:            ML.WEIGHTS (MODEL  %s \n",
            " 180:                ,STRUCT(true AS standardize)\n",
            " 181:            ) w\n",
            " 182:            , unnest(category_weights) as cw\n",
            " 183:            , ML.EVALUATE(MODEL  %s  ) e\n",
            " 184:        order by coalesce(abs(w.weight),cw.weight) desc\n",
            " 185:        limit 8\n",
            " 186:    ; \n",
            " 187:\"\"\"\n",
            " 188:    , (select cast(auc_min_to_fail as string))\n",
            " 189:    , (select model_name) \n",
            " 190:    , (select model_name) \n",
            " 191:); \n",
            " 192:\n",
            " 193:select sql; execute immediate(sql);\n",
            " 194:\n",
            " 195:select * from auc_and_top_contributors; \n",
            " 196:\n",
            " 197:if (SELECT distinct auc from auc_and_top_contributors) > auc_min_to_fail then --auc_min_to_fail then --if auc is above x, then fail the job\n",
            " 198:    select ERROR(\"YIKES! you have a data issue!\");\n",
            " 199:end if;\n",
            "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiA9zQyqrfa5"
      },
      "source": [
        ""
      ]
    }
  ]
}